import _objectSpread from '@babel/runtime/helpers/objectSpread2';
import { KEY_TYPE, LEGACY_NETWORKS_ROUTE_MAP } from '@toruslabs/constants';
import { generatePrivate, getPublic } from '@toruslabs/eccrypto';
import { post, generateJsonRPCObject, get } from '@toruslabs/http-helpers';
import BN from 'bn.js';
import { getRandomBytes } from 'ethereum-cryptography/random';
import { config } from '../config.js';
import { JRPC_METHODS } from '../constants.js';
import log from '../loglevel.js';
import { Some } from '../some.js';
import { generatePrivateKey, thresholdSame, getProxyCoordinatorEndpointIndex, normalizeKeysResult, calculateMedian, normalizeLookUpResult, keccak256, retryCommitment, kCombinations } from './common.js';
import { generateShares, derivePubKey, generateAddressFromPrivKey, generateAddressFromPubKey } from './keyUtils.js';
import { lagrangeInterpolation } from './langrangeInterpolatePoly.js';
import { getOrSetSapphireMetadataNonce, getOrSetNonce, getMetadata, decryptSeedData, getSecpKeyFromEd25519, decryptNodeData, decryptNodeDataWithPadding } from './metadataUtils.js';

const GetPubKeyOrKeyAssign = async params => {
  const {
    endpoints,
    network,
    verifier,
    verifierId,
    extendedVerifierId,
    keyType
  } = params;
  const minThreshold = ~~(endpoints.length / 2) + 1;
  const lookupPromises = endpoints.map(x => post(x, generateJsonRPCObject(JRPC_METHODS.GET_OR_SET_KEY, {
    distributed_metadata: true,
    verifier,
    verifier_id: verifierId.toString(),
    extended_verifier_id: extendedVerifierId,
    one_key_flow: true,
    key_type: keyType,
    fetch_node_index: true,
    client_time: Math.floor(Date.now() / 1000).toString()
  }), {}, {
    logTracingHeader: config.logRequestTracing
  }).catch(err => log.error(`${JRPC_METHODS.GET_OR_SET_KEY} request failed`, err)));
  let nonceResult;
  const nodeIndexes = [];
  const result = await Some(lookupPromises, async lookupResults => {
    const lookupPubKeys = lookupResults.filter(x1 => {
      if (x1 && !x1.error) {
        return x1;
      }
      return false;
    });
    const errorResult = thresholdSame(lookupResults.map(x2 => x2 && x2.error), minThreshold);
    const keyResult = thresholdSame(lookupPubKeys.map(x3 => x3 && normalizeKeysResult(x3.result)), minThreshold);

    // check for nonce result in response if not a extendedVerifierId and not a legacy network
    if (keyResult && !nonceResult && !extendedVerifierId && !LEGACY_NETWORKS_ROUTE_MAP[network]) {
      for (let i = 0; i < lookupResults.length; i++) {
        const x1 = lookupResults[i];
        if (x1 && !x1.error) {
          var _x1$result;
          const currentNodePubKey = x1.result.keys[0].pub_key_X.toLowerCase();
          const thresholdPubKey = keyResult.keys[0].pub_key_X.toLowerCase();
          const pubNonceX = (_x1$result = x1.result) === null || _x1$result === void 0 || (_x1$result = _x1$result.keys[0].nonce_data) === null || _x1$result === void 0 || (_x1$result = _x1$result.pubNonce) === null || _x1$result === void 0 ? void 0 : _x1$result.x;
          if (pubNonceX && currentNodePubKey === thresholdPubKey) {
            nonceResult = x1.result.keys[0].nonce_data;
            break;
          }
        }
      }

      // if nonce result is not returned by nodes, fetch directly from metadata
      if (!nonceResult) {
        const metadataNonceResult = await getOrSetSapphireMetadataNonce(network, keyResult.keys[0].pub_key_X, keyResult.keys[0].pub_key_Y);
        // rechecking nonceResult to avoid promise race condition.
        if (!nonceResult && metadataNonceResult) {
          nonceResult = metadataNonceResult;
          if (nonceResult.nonce) {
            delete nonceResult.nonce;
          }
        }
      }
    }
    const serverTimeOffsets = [];
    // nonceResult must exist except for extendedVerifierId and legacy networks along with keyResult
    if (keyResult && (nonceResult || extendedVerifierId || LEGACY_NETWORKS_ROUTE_MAP[network]) || errorResult) {
      if (keyResult) {
        lookupResults.forEach(x1 => {
          if (x1 && x1.result) {
            const currentNodePubKey = x1.result.keys[0].pub_key_X.toLowerCase();
            const thresholdPubKey = keyResult.keys[0].pub_key_X.toLowerCase();
            // push only those indexes for nodes who are returning pub key matching with threshold pub key.
            // this check is important when different nodes have different keys assigned to a user.
            if (currentNodePubKey === thresholdPubKey) {
              const nodeIndex = Number.parseInt(x1.result.node_index);
              if (nodeIndex) nodeIndexes.push(nodeIndex);
            }
            const serverTimeOffset = x1.result.server_time_offset ? Number.parseInt(x1.result.server_time_offset, 10) : 0;
            serverTimeOffsets.push(serverTimeOffset);
          }
        });
      }
      const serverTimeOffset = keyResult ? calculateMedian(serverTimeOffsets) : 0;
      return Promise.resolve({
        keyResult,
        serverTimeOffset,
        nodeIndexes,
        errorResult,
        nonceResult
      });
    }
    return Promise.reject(new Error(`invalid public key result: ${JSON.stringify(lookupResults)} and nonce result:${JSON.stringify(nonceResult || {})} for verifier: ${verifier}, verifierId: ${verifierId} and extendedVerifierId: ${extendedVerifierId} `));
  });
  return result;
};
const VerifierLookupRequest = async params => {
  const {
    endpoints,
    verifier,
    verifierId,
    keyType
  } = params;
  const minThreshold = ~~(endpoints.length / 2) + 1;
  const lookupPromises = endpoints.map(x => post(x, generateJsonRPCObject(JRPC_METHODS.VERIFIER_LOOKUP, {
    verifier,
    verifier_id: verifierId.toString(),
    key_type: keyType,
    client_time: Math.floor(Date.now() / 1000).toString()
  }), {}, {
    logTracingHeader: config.logRequestTracing
  }).catch(err => log.error(`${JRPC_METHODS.GET_OR_SET_KEY} request failed`, err)));
  const result = await Some(lookupPromises, async lookupResults => {
    const lookupPubKeys = lookupResults.filter(x1 => {
      if (x1 && !x1.error) {
        return x1;
      }
      return false;
    });
    const errorResult = thresholdSame(lookupResults.map(x2 => x2 && x2.error), minThreshold);
    const keyResult = thresholdSame(lookupPubKeys.map(x3 => x3 && normalizeLookUpResult(x3.result)), minThreshold);
    const serverTimeOffsets = [];
    if (keyResult || errorResult) {
      const serverTimeOffset = keyResult ? calculateMedian(serverTimeOffsets) : 0;
      return Promise.resolve({
        keyResult,
        serverTimeOffset,
        errorResult
      });
    }
    return Promise.reject(new Error(`invalid lookup result: ${JSON.stringify(lookupResults)}
        )} for verifier: ${verifier}, verifierId: ${verifierId}`));
  });
  return result;
};
const commitmentRequest = async params => {
  const {
    idToken,
    endpoints,
    indexes,
    keyType,
    verifier,
    verifierParams,
    pubKeyX,
    pubKeyY,
    finalImportedShares,
    overrideExistingKey
  } = params;
  const tokenCommitment = keccak256(Buffer.from(idToken, "utf8"));
  const threeFourthsThreshold = ~~(endpoints.length * 3 / 4) + 1;
  const halfThreshold = ~~(endpoints.length / 2) + 1;
  const promiseArr = [];
  // make commitment requests to endpoints
  for (let i = 0; i < endpoints.length; i += 1) {
    /*
      CommitmentRequestParams struct {
        MessagePrefix      string `json:"messageprefix"`
        TokenCommitment    string `json:"tokencommitment"`
        TempPubX           string `json:"temppubx"`
        TempPubY           string `json:"temppuby"`
        VerifierIdentifier string `json:"verifieridentifier"`
      } 
      */
    const p = () => post(endpoints[i], generateJsonRPCObject(JRPC_METHODS.COMMITMENT_REQUEST, {
      messageprefix: "mug00",
      keytype: keyType,
      tokencommitment: tokenCommitment.slice(2),
      temppubx: pubKeyX,
      temppuby: pubKeyY,
      verifieridentifier: verifier,
      verifier_id: verifierParams.verifier_id,
      extended_verifier_id: verifierParams.extended_verifier_id,
      is_import_key_flow: true
    }), {}, {
      logTracingHeader: config.logRequestTracing
    });
    const r = retryCommitment(p, 4);
    promiseArr.push(r);
  }
  return new Promise((resolve, reject) => {
    // send share request once k + t number of commitment requests have completed
    Some(promiseArr, resultArr => {
      const completedRequests = resultArr.filter(x => {
        if (!x || typeof x !== "object") {
          return false;
        }
        if (x.error) {
          return false;
        }
        return true;
      });
      if (finalImportedShares.length > 0) {
        // this is a optimization is for imported keys
        // for new imported keys registration we need to wait for all nodes to agree on commitment
        // for fetching existing imported keys we can rely on threshold nodes commitment
        if (overrideExistingKey && completedRequests.length === endpoints.length) {
          const requiredNodeResult = completedRequests.find(resp => {
            if (resp) {
              return true;
            }
            return false;
          });
          if (requiredNodeResult) {
            return Promise.resolve(resultArr);
          }
        } else if (!overrideExistingKey && completedRequests.length >= threeFourthsThreshold) {
          const nodeSigs = [];
          for (let i = 0; i < completedRequests.length; i += 1) {
            const x = completedRequests[i];
            if (!x || typeof x !== "object" || x.error) {
              continue;
            }
            if (x) nodeSigs.push(x.result);
          }
          const existingPubKey = thresholdSame(nodeSigs.map(x => x && x.pub_key_x), halfThreshold);
          const proxyEndpointNum = getProxyCoordinatorEndpointIndex(endpoints, verifier, verifierParams.verifier_id);
          // for import shares, proxy node response is required.
          // proxy node returns metadata.
          // if user's account already
          const requiredNodeIndex = indexes[proxyEndpointNum].toString(10);

          // if not a existing key we need to wait for nodes to agree on commitment
          if (existingPubKey || !existingPubKey && completedRequests.length === endpoints.length) {
            const requiredNodeResult = completedRequests.find(resp => {
              var _resp$result;
              if (resp && ((_resp$result = resp.result) === null || _resp$result === void 0 ? void 0 : _resp$result.nodeindex) === requiredNodeIndex) {
                return true;
              }
              return false;
            });
            if (requiredNodeResult) {
              return Promise.resolve(resultArr);
            }
          }
        }
      } else if (completedRequests.length >= threeFourthsThreshold) {
        // this case is for dkg keys
        const requiredNodeResult = completedRequests.find(resp => {
          if (resp) {
            return true;
          }
          return false;
        });
        if (requiredNodeResult) {
          return Promise.resolve(resultArr);
        }
      }
      return Promise.reject(new Error(`invalid commitment results ${JSON.stringify(resultArr)}`));
    }).then(resultArr => {
      return resolve(resultArr);
    }).catch(reject);
  });
};
async function retrieveOrImportShare(params) {
  const {
    legacyMetadataHost,
    enableOneKey,
    ecCurve,
    keyType,
    allowHost,
    network,
    clientId,
    endpoints,
    nodePubkeys,
    indexes,
    verifier,
    verifierParams,
    idToken,
    overrideExistingKey,
    newImportedShares,
    extraParams,
    useDkg = true,
    serverTimeOffset,
    checkCommitment = true
  } = params;
  await get(allowHost, {
    headers: {
      verifier,
      verifierid: verifierParams.verifier_id,
      network,
      clientid: clientId,
      enablegating: "true"
    }
  }, {
    useAPIKey: true
  });

  // generate temporary private and public key that is used to secure receive shares
  const sessionAuthKey = generatePrivate();
  const pubKey = getPublic(sessionAuthKey).toString("hex");
  const sessionPubX = pubKey.slice(2, 66);
  const sessionPubY = pubKey.slice(66);
  let finalImportedShares = [];
  const halfThreshold = ~~(endpoints.length / 2) + 1;
  if ((newImportedShares === null || newImportedShares === void 0 ? void 0 : newImportedShares.length) > 0) {
    if (newImportedShares.length !== endpoints.length) {
      throw new Error("Invalid imported shares length");
    }
    finalImportedShares = newImportedShares;
  } else if (!useDkg) {
    const bufferKey = keyType === KEY_TYPE.SECP256K1 ? generatePrivateKey(ecCurve, Buffer) : await getRandomBytes(32);
    const generatedShares = await generateShares(ecCurve, keyType, serverTimeOffset, indexes, nodePubkeys, Buffer.from(bufferKey));
    finalImportedShares = [...finalImportedShares, ...generatedShares];
  }
  let commitmentRequestResult = [];
  let isExistingKey;
  const nodeSigs = [];
  if (checkCommitment) {
    commitmentRequestResult = await commitmentRequest({
      idToken,
      endpoints,
      indexes,
      keyType,
      verifier,
      verifierParams,
      pubKeyX: sessionPubX,
      pubKeyY: sessionPubY,
      finalImportedShares,
      overrideExistingKey
    });
    for (let i = 0; i < commitmentRequestResult.length; i += 1) {
      const x = commitmentRequestResult[i];
      if (!x || typeof x !== "object" || x.error) {
        continue;
      }
      if (x) nodeSigs.push(x.result);
    }
    // if user's account already
    isExistingKey = !!thresholdSame(nodeSigs.map(x => x && x.pub_key_x), halfThreshold);
  } else if (!checkCommitment && finalImportedShares.length > 0) {
    // in case not allowed to override existing key for import request
    // check if key exists
    if (!overrideExistingKey) {
      var _keyLookupResult$erro, _keyLookupResult$keyR;
      const keyLookupResult = await VerifierLookupRequest({
        endpoints,
        verifier,
        verifierId: verifierParams.verifier_id,
        keyType
      });
      if (keyLookupResult.errorResult && !((_keyLookupResult$erro = keyLookupResult.errorResult) !== null && _keyLookupResult$erro !== void 0 && (_keyLookupResult$erro = _keyLookupResult$erro.data) !== null && _keyLookupResult$erro !== void 0 && _keyLookupResult$erro.includes("Verifier + VerifierID has not yet been assigned"))) {
        throw new Error(`node results do not match at first lookup ${JSON.stringify(keyLookupResult.keyResult || {})}, ${JSON.stringify(keyLookupResult.errorResult || {})}`);
      }
      if (((_keyLookupResult$keyR = keyLookupResult.keyResult) === null || _keyLookupResult$keyR === void 0 || (_keyLookupResult$keyR = _keyLookupResult$keyR.keys) === null || _keyLookupResult$keyR === void 0 ? void 0 : _keyLookupResult$keyR.length) > 0) {
        isExistingKey = !!keyLookupResult.keyResult.keys[0];
      }
    }
  }
  const promiseArrRequest = [];
  const canImportedShares = overrideExistingKey || !useDkg && !isExistingKey;
  if (canImportedShares) {
    const proxyEndpointNum = getProxyCoordinatorEndpointIndex(endpoints, verifier, verifierParams.verifier_id);
    const items = [];
    for (let i = 0; i < endpoints.length; i += 1) {
      const importedShare = finalImportedShares[i];
      if (!importedShare) {
        throw new Error(`invalid imported share at index ${i}`);
      }
      items.push(_objectSpread(_objectSpread({}, verifierParams), {}, {
        idtoken: idToken,
        nodesignatures: nodeSigs,
        verifieridentifier: verifier,
        pub_key_x: importedShare.oauth_pub_key_x,
        pub_key_y: importedShare.oauth_pub_key_y,
        signing_pub_key_x: importedShare.signing_pub_key_x,
        signing_pub_key_y: importedShare.signing_pub_key_y,
        encrypted_share: importedShare.encrypted_share,
        encrypted_share_metadata: importedShare.encrypted_share_metadata,
        node_index: importedShare.node_index,
        key_type: importedShare.key_type,
        nonce_data: importedShare.nonce_data,
        nonce_signature: importedShare.nonce_signature,
        sss_endpoint: endpoints[i]
      }, extraParams));
    }
    const p = post(endpoints[proxyEndpointNum], generateJsonRPCObject(JRPC_METHODS.IMPORT_SHARES, {
      encrypted: "yes",
      use_temp: true,
      verifieridentifier: verifier,
      temppubx: nodeSigs.length === 0 && !checkCommitment ? sessionPubX : "",
      // send session pub key x only if node signatures are not available (Ie. in non commitment flow)
      temppuby: nodeSigs.length === 0 && !checkCommitment ? sessionPubY : "",
      // send session pub key y only if node signatures are not available (Ie. in non commitment flow)
      item: items,
      key_type: keyType,
      one_key_flow: true
    }), {}, {
      logTracingHeader: config.logRequestTracing
    }).catch(err => log.error("share req", err));
    promiseArrRequest.push(p);
  } else {
    for (let i = 0; i < endpoints.length; i += 1) {
      const p = post(endpoints[i], generateJsonRPCObject(JRPC_METHODS.GET_SHARE_OR_KEY_ASSIGN, {
        encrypted: "yes",
        use_temp: true,
        key_type: keyType,
        distributed_metadata: true,
        verifieridentifier: verifier,
        temppubx: nodeSigs.length === 0 && !checkCommitment ? sessionPubX : "",
        // send session pub key x only if node signatures are not available (Ie. in non commitment flow)
        temppuby: nodeSigs.length === 0 && !checkCommitment ? sessionPubY : "",
        // send session pub key y only if node signatures are not available (Ie. in non commitment flow)
        item: [_objectSpread(_objectSpread({}, verifierParams), {}, {
          idtoken: idToken,
          key_type: keyType,
          nodesignatures: nodeSigs,
          verifieridentifier: verifier
        }, extraParams)],
        client_time: Math.floor(Date.now() / 1000).toString(),
        one_key_flow: true
      }), {}, {
        logTracingHeader: config.logRequestTracing
      });
      promiseArrRequest.push(p);
    }
  }
  return Some(promiseArrRequest, async (shareResponseResult, sharedState) => {
    let thresholdNonceData;
    let shareResponses = [];
    // for import shares case, where result is an array
    if (shareResponseResult.length === 1 && shareResponseResult[0] && Array.isArray(shareResponseResult[0].result)) {
      // this is for import shares
      const importedSharesResult = shareResponseResult[0];
      shareResponseResult[0].result.forEach(res => {
        shareResponses.push({
          id: importedSharesResult.id,
          jsonrpc: "2.0",
          result: res,
          error: importedSharesResult.error
        });
      });
    } else {
      shareResponses = shareResponseResult;
    }
    // check if threshold number of nodes have returned the same user public key
    const completedRequests = shareResponses.filter(x => {
      if (!x || typeof x !== "object") {
        return false;
      }
      if (x.error) {
        return false;
      }
      return true;
    });
    const pubkeys = shareResponses.map(x => {
      if (x && x.result && x.result.keys[0].public_key) {
        return x.result.keys[0].public_key;
      }
      return undefined;
    });
    const thresholdPublicKey = thresholdSame(pubkeys, halfThreshold);
    if (!thresholdPublicKey) {
      throw new Error("invalid result from nodes, threshold number of public key results are not matching");
    }
    shareResponses.forEach(x => {
      const requiredShareResponse = x && x.result && x.result.keys[0].public_key && x.result.keys[0];
      if (requiredShareResponse && !thresholdNonceData && !verifierParams.extended_verifier_id) {
        var _requiredShareRespons;
        const currentPubKey = requiredShareResponse.public_key;
        const pubNonce = (_requiredShareRespons = requiredShareResponse.nonce_data) === null || _requiredShareRespons === void 0 || (_requiredShareRespons = _requiredShareRespons.pubNonce) === null || _requiredShareRespons === void 0 ? void 0 : _requiredShareRespons.x;
        if (pubNonce && currentPubKey.X === thresholdPublicKey.X) {
          thresholdNonceData = requiredShareResponse.nonce_data;
        }
      }
    });
    const thresholdReqCount = canImportedShares ? endpoints.length : halfThreshold;
    // optimistically run lagrange interpolation once threshold number of shares have been received
    // this is matched against the user public key to ensure that shares are consistent
    // Note: no need of thresholdMetadataNonce for extended_verifier_id key
    if (completedRequests.length >= thresholdReqCount && thresholdPublicKey) {
      const sharePromises = [];
      const sessionTokenSigPromises = [];
      const sessionTokenPromises = [];
      const nodeIndexes = [];
      const sessionTokenData = [];
      const isNewKeyResponses = [];
      const serverTimeOffsetResponses = [];
      for (let i = 0; i < completedRequests.length; i += 1) {
        var _currentShareResponse;
        const currentShareResponse = completedRequests[i];
        const {
          session_tokens: sessionTokens,
          session_token_metadata: sessionTokenMetadata,
          session_token_sigs: sessionTokenSigs,
          session_token_sig_metadata: sessionTokenSigMetadata,
          keys,
          is_new_key: isNewKey,
          server_time_offset: serverTimeOffsetResponse
        } = currentShareResponse.result;
        isNewKeyResponses.push({
          isNewKey,
          publicKey: ((_currentShareResponse = currentShareResponse.result) === null || _currentShareResponse === void 0 || (_currentShareResponse = _currentShareResponse.keys[0]) === null || _currentShareResponse === void 0 || (_currentShareResponse = _currentShareResponse.public_key) === null || _currentShareResponse === void 0 ? void 0 : _currentShareResponse.X) || ""
        });
        serverTimeOffsetResponses.push(serverTimeOffsetResponse || "0");
        if ((sessionTokenSigs === null || sessionTokenSigs === void 0 ? void 0 : sessionTokenSigs.length) > 0) {
          var _sessionTokenSigMetad;
          // decrypt sessionSig if enc metadata is sent
          if (sessionTokenSigMetadata && (_sessionTokenSigMetad = sessionTokenSigMetadata[0]) !== null && _sessionTokenSigMetad !== void 0 && _sessionTokenSigMetad.ephemPublicKey) {
            sessionTokenSigPromises.push(decryptNodeData(sessionTokenSigMetadata[0], sessionTokenSigs[0], sessionAuthKey).catch(err => log.error("session sig decryption", err)));
          } else {
            sessionTokenSigPromises.push(Promise.resolve(Buffer.from(sessionTokenSigs[0], "hex")));
          }
        } else {
          sessionTokenSigPromises.push(Promise.resolve(undefined));
        }
        if ((sessionTokens === null || sessionTokens === void 0 ? void 0 : sessionTokens.length) > 0) {
          var _sessionTokenMetadata;
          // decrypt session token if enc metadata is sent
          if (sessionTokenMetadata && (_sessionTokenMetadata = sessionTokenMetadata[0]) !== null && _sessionTokenMetadata !== void 0 && _sessionTokenMetadata.ephemPublicKey) {
            sessionTokenPromises.push(decryptNodeData(sessionTokenMetadata[0], sessionTokens[0], sessionAuthKey).catch(err => log.error("session token sig decryption", err)));
          } else {
            sessionTokenPromises.push(Promise.resolve(Buffer.from(sessionTokens[0], "base64")));
          }
        } else {
          sessionTokenPromises.push(Promise.resolve(undefined));
        }
        if ((keys === null || keys === void 0 ? void 0 : keys.length) > 0) {
          const latestKey = currentShareResponse.result.keys[0];
          nodeIndexes.push(new BN(latestKey.node_index));
          if (latestKey.share_metadata) {
            sharePromises.push(decryptNodeDataWithPadding(latestKey.share_metadata, Buffer.from(latestKey.share, "base64").toString("binary"), sessionAuthKey).catch(err => log.error("share decryption", err)));
          }
        } else {
          nodeIndexes.push(undefined);
          sharePromises.push(Promise.resolve(undefined));
        }
      }
      const allPromises = await Promise.all(sharePromises.concat(sessionTokenSigPromises).concat(sessionTokenPromises));
      const sharesResolved = allPromises.slice(0, sharePromises.length);
      const sessionSigsResolved = allPromises.slice(sharePromises.length, sharePromises.length + sessionTokenSigPromises.length);
      const sessionTokensResolved = allPromises.slice(sharePromises.length + sessionTokenSigPromises.length, allPromises.length);
      const validSigs = sessionSigsResolved.filter(sig => {
        if (sig) {
          return true;
        }
        return false;
      });
      if (!verifierParams.extended_verifier_id && validSigs.length < halfThreshold) {
        throw new Error(`Insufficient number of signatures from nodes, required: ${halfThreshold}, found: ${validSigs.length}`);
      }
      const validTokens = sessionTokensResolved.filter(token => {
        if (token) {
          return true;
        }
        return false;
      });
      if (!verifierParams.extended_verifier_id && validTokens.length < halfThreshold) {
        throw new Error(`Insufficient number of session tokens from nodes, required: ${halfThreshold}, found: ${validTokens.length}`);
      }
      sessionTokensResolved.forEach((x, index) => {
        if (!x || !sessionSigsResolved[index]) sessionTokenData.push(undefined);else sessionTokenData.push({
          token: x.toString("base64"),
          signature: sessionSigsResolved[index].toString("hex"),
          node_pubx: completedRequests[index].result.node_pubx,
          node_puby: completedRequests[index].result.node_puby
        });
      });
      if (sharedState.resolved) return undefined;
      const decryptedShares = sharesResolved.reduce((acc, curr, index) => {
        if (curr) {
          acc.push({
            index: nodeIndexes[index],
            value: new BN(curr)
          });
        }
        return acc;
      }, []);
      // run lagrange interpolation on all subsets, faster in the optimistic scenario than berlekamp-welch due to early exit
      const allCombis = kCombinations(decryptedShares.length, halfThreshold);
      let privateKey = null;
      for (let j = 0; j < allCombis.length; j += 1) {
        const currentCombi = allCombis[j];
        const currentCombiShares = decryptedShares.filter((_, index) => currentCombi.includes(index));
        const shares = currentCombiShares.map(x => x.value);
        const indices = currentCombiShares.map(x => x.index);
        const derivedPrivateKey = lagrangeInterpolation(ecCurve, shares, indices);
        if (!derivedPrivateKey) continue;
        const decryptedPubKey = derivePubKey(ecCurve, derivedPrivateKey);
        const decryptedPubKeyX = decryptedPubKey.getX();
        const decryptedPubKeyY = decryptedPubKey.getY();
        if (decryptedPubKeyX.cmp(new BN(thresholdPublicKey.X, 16)) === 0 && decryptedPubKeyY.cmp(new BN(thresholdPublicKey.Y, 16)) === 0) {
          privateKey = derivedPrivateKey;
          break;
        }
      }
      if (privateKey === undefined || privateKey === null) {
        throw new Error("could not derive private key");
      }
      let isNewKey = false;
      isNewKeyResponses.forEach(x => {
        if (x.isNewKey === "true" && x.publicKey.toLowerCase() === thresholdPublicKey.X.toLowerCase()) {
          isNewKey = true;
        }
      });

      // Convert each string timestamp to a number
      const serverOffsetTimes = serverTimeOffsetResponses.map(timestamp => Number.parseInt(timestamp, 10));
      return {
        privateKey,
        sessionTokenData,
        thresholdNonceData,
        nodeIndexes,
        thresholdPubKey: thresholdPublicKey,
        isNewKey,
        serverTimeOffsetResponse: serverTimeOffset || calculateMedian(serverOffsetTimes)
      };
    }
    if (completedRequests.length < thresholdReqCount) {
      throw new Error(`Waiting for results from more nodes, pending: ${thresholdReqCount - completedRequests.length}`);
    }
    throw new Error(`Invalid results, threshold pub key: ${thresholdPublicKey}, nonce data found: ${!!thresholdNonceData}, extended verifierId: ${verifierParams.extended_verifier_id}`);
  }).then(async res => {
    var _nonceResult;
    const {
      privateKey,
      thresholdPubKey,
      sessionTokenData,
      nodeIndexes,
      thresholdNonceData,
      isNewKey,
      serverTimeOffsetResponse
    } = res;
    let nonceResult = thresholdNonceData;
    if (!privateKey) throw new Error("Invalid private key returned");
    const oAuthKey = privateKey;
    const oAuthPubKey = derivePubKey(ecCurve, oAuthKey);
    const oAuthPubkeyX = oAuthPubKey.getX().toString("hex", 64);
    const oAuthPubkeyY = oAuthPubKey.getY().toString("hex", 64);

    // if both thresholdNonceData and extended_verifier_id are not available
    // then we need to throw other wise address would be incorrect.
    if (!nonceResult && !verifierParams.extended_verifier_id && !LEGACY_NETWORKS_ROUTE_MAP[network]) {
      // NOTE: dont use padded pub key anywhere in metadata apis, send pub keys as is received from nodes.
      const metadataNonceResult = await getOrSetSapphireMetadataNonce(network, thresholdPubKey.X, thresholdPubKey.Y, serverTimeOffset, oAuthKey);
      // rechecking nonceResult to avoid promise race condition.
      if (metadataNonceResult && !thresholdNonceData) {
        nonceResult = metadataNonceResult;
      } else {
        throw new Error(`invalid metadata result from nodes, nonce metadata is empty for verifier: ${verifier} and verifierId: ${verifierParams.verifier_id}`);
      }
    }
    let metadataNonce = new BN((_nonceResult = nonceResult) !== null && _nonceResult !== void 0 && _nonceResult.nonce ? nonceResult.nonce.padStart(64, "0") : "0", "hex");
    let finalPubKey;
    let pubNonce;
    let typeOfUser = "v1";
    // extended_verifier_id is only exception for torus-test-health verifier
    // otherwise extended verifier id should not even return shares.
    if (verifierParams.extended_verifier_id) {
      typeOfUser = "v2";
      // for tss key no need to add pub nonce
      finalPubKey = ecCurve.keyFromPublic({
        x: oAuthPubkeyX,
        y: oAuthPubkeyY
      }).getPublic();
    } else if (LEGACY_NETWORKS_ROUTE_MAP[network]) {
      if (enableOneKey) {
        nonceResult = await getOrSetNonce(legacyMetadataHost, ecCurve, serverTimeOffsetResponse, oAuthPubkeyX, oAuthPubkeyY, oAuthKey, !isNewKey);
        metadataNonce = new BN(nonceResult.nonce || "0", 16);
        typeOfUser = nonceResult.typeOfUser;
        if (typeOfUser === "v2") {
          pubNonce = {
            X: nonceResult.pubNonce.x,
            Y: nonceResult.pubNonce.y
          };
          finalPubKey = ecCurve.keyFromPublic({
            x: oAuthPubkeyX,
            y: oAuthPubkeyY
          }).getPublic().add(ecCurve.keyFromPublic({
            x: nonceResult.pubNonce.x,
            y: nonceResult.pubNonce.y
          }).getPublic());
        } else {
          typeOfUser = "v1";
          // for imported keys in legacy networks
          metadataNonce = await getMetadata(legacyMetadataHost, {
            pub_key_X: oAuthPubkeyX,
            pub_key_Y: oAuthPubkeyY
          });
          const privateKeyWithNonce = oAuthKey.add(metadataNonce).umod(ecCurve.n);
          finalPubKey = ecCurve.keyFromPrivate(privateKeyWithNonce.toString(16, 64), "hex").getPublic();
        }
      } else {
        typeOfUser = "v1";
        // for imported keys in legacy networks
        metadataNonce = await getMetadata(legacyMetadataHost, {
          pub_key_X: oAuthPubkeyX,
          pub_key_Y: oAuthPubkeyY
        });
        const privateKeyWithNonce = oAuthKey.add(metadataNonce).umod(ecCurve.n);
        finalPubKey = ecCurve.keyFromPrivate(privateKeyWithNonce.toString(16, 64), "hex").getPublic();
      }
    } else {
      typeOfUser = "v2";
      finalPubKey = ecCurve.keyFromPublic({
        x: oAuthPubkeyX,
        y: oAuthPubkeyY
      }).getPublic().add(ecCurve.keyFromPublic({
        x: nonceResult.pubNonce.x,
        y: nonceResult.pubNonce.y
      }).getPublic());
      pubNonce = {
        X: nonceResult.pubNonce.x,
        Y: nonceResult.pubNonce.y
      };
    }
    if (!finalPubKey) {
      throw new Error("Invalid public key, this might be a bug, please report this to web3auth team");
    }
    let finalPrivKey = ""; // it is empty for v2 user upgraded to 2/n
    let isUpgraded = false;
    const oAuthKeyAddress = generateAddressFromPrivKey(keyType, oAuthKey);
    // deriving address from pub key coz pubkey is always available
    // but finalPrivKey won't be available for  v2 user upgraded to 2/n
    const finalWalletAddress = generateAddressFromPubKey(keyType, finalPubKey.getX(), finalPubKey.getY());
    let keyWithNonce = "";
    if (typeOfUser === "v1") {
      isUpgraded = null;
    } else if (typeOfUser === "v2") {
      isUpgraded = metadataNonce.eq(new BN("0"));
    }
    if (typeOfUser === "v1" || typeOfUser === "v2" && metadataNonce.gt(new BN(0))) {
      const privateKeyWithNonce = oAuthKey.add(metadataNonce).umod(ecCurve.n);
      keyWithNonce = privateKeyWithNonce.toString("hex", 64);
    }
    if (keyType === KEY_TYPE.SECP256K1) {
      finalPrivKey = keyWithNonce;
    } else if (keyType === KEY_TYPE.ED25519) {
      if (keyWithNonce && !nonceResult.seed) {
        throw new Error("Invalid data, seed data is missing for ed25519 key, Please report this bug");
      } else if (keyWithNonce && nonceResult.seed) {
        // console.log("nonceResult.seed", nonceResult.seed, keyWithNonce);
        const decryptedSeed = await decryptSeedData(nonceResult.seed, new BN(keyWithNonce, "hex"));
        finalPrivKey = decryptedSeed.toString("hex");
      }
    } else {
      throw new Error(`Invalid keyType: ${keyType}`);
    }
    let postboxKey = oAuthKey;
    let postboxPubX = oAuthPubkeyX;
    let postboxPubY = oAuthPubkeyY;
    if (keyType === KEY_TYPE.ED25519) {
      const {
        scalar,
        point
      } = getSecpKeyFromEd25519(privateKey);
      postboxKey = scalar;
      postboxPubX = point.getX().toString(16, 64);
      postboxPubY = point.getY().toString(16, 64);
      if (thresholdPubKey.SignerX.padStart(64, "0") !== postboxPubX || thresholdPubKey.SignerY.padStart(64, "0") !== postboxPubY) {
        throw new Error("Invalid postbox key");
      }
    }
    // return reconstructed private key and ethereum address
    return {
      finalKeyData: {
        walletAddress: finalWalletAddress,
        X: finalPubKey.getX().toString(16, 64),
        // this is final pub x user before and after updating to 2/n
        Y: finalPubKey.getY().toString(16, 64),
        // this is final pub y user before and after updating to 2/n
        privKey: finalPrivKey
      },
      oAuthKeyData: {
        walletAddress: oAuthKeyAddress,
        X: oAuthPubkeyX,
        Y: oAuthPubkeyY,
        privKey: oAuthKey.toString("hex", 64)
      },
      postboxKeyData: {
        privKey: postboxKey.toString("hex", 64),
        X: postboxPubX,
        Y: postboxPubY
      },
      sessionData: {
        sessionTokenData,
        sessionAuthKey: sessionAuthKey.toString("hex").padStart(64, "0")
      },
      metadata: {
        pubNonce,
        nonce: metadataNonce,
        typeOfUser,
        upgraded: isUpgraded,
        serverTimeOffset: serverTimeOffsetResponse
      },
      nodesData: {
        nodeIndexes: nodeIndexes.map(x => x.toNumber())
      }
    };
  });
}

export { GetPubKeyOrKeyAssign, VerifierLookupRequest, retrieveOrImportShare };
